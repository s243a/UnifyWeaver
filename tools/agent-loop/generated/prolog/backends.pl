%% backends — Backend definitions and API dispatch
%% Auto-generated by agent_loop_module.pl
%% DO NOT EDIT — regenerate with:
%%   swipl -g "generate_all(prolog), halt" agent_loop_module.pl

:- module(backends, [
    agent_backend/2,
    backend_factory/2,
    backend_factory_order/1,
    cli_fallbacks/2,
    create_backend/3,
    send_request/4
]).

:- use_module(library(http/http_open)).
:- use_module(library(http/http_header)).
:- use_module(library(json)).
:- use_module(library(process)).
:- use_module(library(readutil)).

%% agent_backend(+Name, +Properties)
agent_backend(coro, [type(cli), class_name('CoroBackend'), command("claude"), args(["--verbose"]), description("Coro-code CLI backend using single-task mode"), context_format(conversation_history), output_parser(coro_parser), supports_streaming(false), module_imports(['json as _json',os,subprocess,re,tempfile]), class_docstring('Coro-code CLI backend using single-task mode.'), display_name('Coro ({self.command})'), helper_fragments([])]).
agent_backend(claude_code, [type(cli), class_name('ClaudeCodeBackend'), command("claude"), args(["-p","--model"]), description("Claude Code CLI backend using print mode"), default_model("sonnet"), models(["sonnet","opus","haiku"]), context_format(conversation_history), supports_streaming(false), module_imports([json,os,subprocess]), class_docstring('Claude Code CLI backend with streaming JSON output.'), display_name('Claude Code ({self.model})'), helper_fragments([stream_json_return,describe_tool_call_claude_code,format_prompt])]).
agent_backend(gemini, [type(cli), class_name('GeminiBackend'), command("gemini"), args(["-p","-m","--output-format","text"]), description("Gemini CLI backend"), default_model("gemini-2.5-flash"), context_format(conversation_history), supports_streaming(false), module_imports([json,os,subprocess,sys]), class_docstring('Gemini CLI backend with streaming JSON output.'), display_name('Gemini ({self.model})'), helper_fragments([stream_json_return,describe_tool_call_gemini,format_prompt])]).
agent_backend(claude_api, [type(api), class_name('ClaudeAPIBackend'), endpoint("https://api.anthropic.com/v1/messages"), model("claude-sonnet-4-20250514"), auth_env("ANTHROPIC_API_KEY"), auth_header("x-api-key"), description("Anthropic Claude API backend"), context_format(messages_array), supports_tools(true), supports_streaming(true), optional_import(true), module_imports([os]), sdk_guard(anthropic), class_docstring('Anthropic Claude API backend.'), display_name('Claude API ({self.model})'), helper_fragments([extract_tool_calls_anthropic])]).
agent_backend(openai_api, [type(api), class_name('OpenAIBackend'), file_name(openai_api), endpoint("https://api.openai.com/v1/chat/completions"), model("gpt-4o"), auth_env("OPENAI_API_KEY"), auth_header("Authorization"), auth_prefix("Bearer "), description("OpenAI API backend"), context_format(messages_array), supports_tools(true), supports_streaming(true), optional_import(true), module_imports([os]), sdk_guard(openai), class_docstring('OpenAI API backend (GPT-4, GPT-3.5, etc.).'), display_name('OpenAI ({self.model})'), helper_fragments([extract_tool_calls_openai])]).
agent_backend(ollama_api, [type(api), class_name('OllamaAPIBackend'), endpoint("http://localhost:11434/api/chat"), model("llama3"), description("Ollama REST API backend for local models"), default_host("localhost"), default_port(11434), context_format(messages_array), auth_required(false), supports_streaming(true), module_imports([json,'urllib.request','urllib.error']), class_docstring('Ollama REST API backend for local models.'), display_name('Ollama API ({self.model}@{self.host}:{self.port})'), helper_fragments([list_models_api])]).
agent_backend(ollama_cli, [type(cli), class_name('OllamaCLIBackend'), command("ollama"), args(["run"]), description("Ollama CLI backend using 'ollama run' command"), default_model("llama3"), context_format(conversation_history), supports_streaming(false), module_imports([subprocess]), class_docstring('Ollama CLI backend using \'ollama run\' command.'), display_name('Ollama CLI ({self.model})'), helper_fragments([format_prompt,clean_output_simple,list_models_cli])]).
agent_backend(openrouter_api, [type(api), class_name('OpenRouterBackend'), endpoint("https://openrouter.ai/api/v1/chat/completions"), model("anthropic/claude-sonnet-4-20250514"), auth_env("OPENROUTER_API_KEY"), auth_header("Authorization"), auth_prefix("Bearer "), description("OpenRouter API backend with model routing"), context_format(messages_array), supports_tools(true), supports_streaming(true), module_imports([json,os,sys,'urllib.request','urllib.error']), class_docstring('OpenRouter API backend (OpenAI-compatible, no pip deps).'), display_name('OpenRouter ({self.model})'), helper_fragments([supports_streaming_true,sse_streaming_openrouter,describe_tool_call_openrouter])]).

%% backend_factory(+Name, +FactorySpec)
backend_factory(coro, [resolve_type(cli), class_name('CoroBackend'), default_command(coro), constructor_args([arg(command,cmd),arg(no_fallback,no_fallback),arg_expr(max_context_tokens,'agent_config.max_context_tokens\n            if agent_config.max_context_tokens != 100000 else 0')])]).
backend_factory('claude-code', [resolve_type(cli), class_name('ClaudeCodeBackend'), import_from(backends), default_command(claude), default_model(sonnet), constructor_args([arg(command,cmd),arg_model])]).
backend_factory(gemini, [resolve_type(cli), class_name('GeminiBackend'), import_from(backends), default_command(gemini), default_model('gemini-3-flash-preview'), constructor_args([arg(command,cmd),arg_model,arg(sandbox,sandbox),arg(approval_mode,approval_mode),arg_expr(allowed_tools,'allowed_tools or []')])]).
backend_factory(claude, [resolve_type(api), class_name('ClaudeAPIBackend'), import_from(backends), default_model('claude-sonnet-4-20250514'), constructor_args([arg(api_key,api_key),arg_model,arg(system_prompt,system_prompt)])]).
backend_factory(openai, [resolve_type(api), class_name('OpenAIBackend'), import_from(backends), default_model('gpt-4o'), constructor_args([arg(api_key,api_key),arg_model,arg(system_prompt,system_prompt),arg_expr(base_url,'agent_config.extra.get(\'base_url\')')])]).
backend_factory(openrouter, [resolve_type(openrouter), class_name('OpenRouterBackend'), import_from(backends), constructor_args([arg(api_key,api_key),arg_expr(model,'agent_config.model or cascade.get(\'model\')'),arg_expr(base_url,'(agent_config.extra.get(\'base_url\')\n                      or cascade.get(\'base_url\', \'https://openrouter.ai/api/v1\'))'),arg(system_prompt,system_prompt),arg_trailing(tools,tool_schemas)])]).
backend_factory('ollama-api', [resolve_type(api_local), class_name('OllamaAPIBackend'), import_from(backends), default_model(llama3), constructor_args([arg_expr(host,'agent_config.host or \'localhost\''),arg_expr(port,'agent_config.port or 11434'),arg_model,arg(system_prompt,system_prompt),arg(timeout,'agent_config.timeout')])]).
backend_factory('ollama-cli', [resolve_type(cli), class_name('OllamaCLIBackend'), import_from(backends), default_command(ollama), default_model(llama3), constructor_args([arg(command,cmd),arg_model,arg(timeout,'agent_config.timeout')])]).

backend_factory_order([coro, 'claude-code', gemini, claude, openai, openrouter, 'ollama-api', 'ollama-cli']).

%% cli_fallbacks(+BackendName, +FallbackList)
cli_fallbacks(coro, []).
cli_fallbacks('claude-code', []).
cli_fallbacks(gemini, []).
cli_fallbacks('ollama-cli', []).

%% Create a backend configuration from factory specs
create_backend(Name, Options, Backend) :-
    (backend_factory(Name, Spec) ->
        Backend = backend{name: Name, spec: Spec, options: Options}
    ; format(atom(Err), "Unknown backend: ~w", [Name]),
      throw(error(Err))).

%% Send a request to a backend and normalize the response
send_request(Backend, Messages, Tools, Response) :-
    get_dict(spec, Backend, Spec),
    member(resolve_type(Type), Spec),
    catch(
        (send_request_raw(Type, Backend, Messages, Tools, RawResponse),
         extract_response(RawResponse, Response)),
        Error,
        (format(atom(ErrMsg), "Request error: ~w", [Error]),
         Response = _{content: ErrMsg, tool_calls: []})).

%% API backend: HTTP JSON request
send_request_raw(api, Backend, Messages, Tools, Response) :-
    get_dict(spec, Backend, Spec),
    member(endpoint(URL), Spec),
    member(model(Model), Spec),
    get_dict(options, Backend, Opts),
    (member(api_key(Key), Opts) -> true ; Key = none),
    (member(auth_header(AuthH), Spec) -> true ; AuthH = "Authorization"),
    (member(auth_prefix(AuthP), Spec) -> true ; AuthP = "Bearer "),
    atom_concat(AuthP, Key, AuthVal),
    Body = json(_{model: Model, messages: Messages, tools: Tools}),
    setup_call_cleanup(
        http_open(URL, In, [
            method(post),
            request_header(AuthH=AuthVal),
            request_header('Content-Type'='application/json'),
            post(Body)
        ]),
        json_read_dict(In, Response),
        close(In)).
send_request_raw(openrouter, B, M, T, R) :- send_request_raw(api, B, M, T, R).
send_request_raw(api_local, B, M, T, R) :- send_request_raw(api, B, M, T, R).

%% CLI backend: subprocess
send_request_raw(cli, Backend, Messages, _Tools, Response) :-
    get_dict(spec, Backend, Spec),
    member(command(Cmd), Spec),
    member(args(BaseArgs), Spec),
    last(Messages, LastMsg),
    get_dict(content, LastMsg, Prompt),
    append(BaseArgs, [Prompt], AllArgs),
    setup_call_cleanup(
        process_create(path(Cmd), AllArgs,
            [stdout(pipe(Out)), stderr(pipe(Err)), process(PID)]),
        (read_string(Out, _, StdOut),
         read_string(Err, _, StdErr),
         process_wait(PID, Status)),
        (close(Out), close(Err))),
    (Status = exit(0) ->
        Response = _{content: StdOut, tool_calls: []}
    ; format(atom(ErrMsg), "Backend exited with ~w: ~w", [Status, StdErr]),
        Response = _{content: ErrMsg, tool_calls: []}).

%% Extract and normalize API response to _{content, tool_calls}
extract_response(Raw, Normalized) :-
    (get_dict(choices, Raw, Choices) ->
        %% OpenAI/OpenRouter format
        Choices = [FirstChoice|_],
        get_dict(message, FirstChoice, Msg),
        (get_dict(content, Msg, Content0) -> true ; Content0 = ""),
        (Content0 = null -> Content = "" ; Content = Content0),
        (get_dict(tool_calls, Msg, TCs) ->
            maplist(normalize_openai_tc, TCs, ToolCalls)
        ; ToolCalls = []),
        Normalized = _{content: Content, tool_calls: ToolCalls}
    ; get_dict(content, Raw, ContentList), is_list(ContentList) ->
        %% Anthropic format — content is a list of blocks
        extract_anthropic(ContentList, Content, ToolCalls),
        Normalized = _{content: Content, tool_calls: ToolCalls}
    ; %% Already normalized (e.g., CLI backend)
        Normalized = Raw).

%% Normalize OpenAI tool call format
normalize_openai_tc(TC, Normalized) :-
    get_dict(id, TC, Id),
    get_dict(function, TC, Func),
    get_dict(name, Func, Name),
    get_dict(arguments, Func, ArgsJson),
    (is_dict(ArgsJson) -> Args = ArgsJson
    ; atom_to_term(ArgsJson, Args, _)),
    Normalized = _{id: Id, name: Name, arguments: Args}.

%% Extract Anthropic content blocks
extract_anthropic(Blocks, Content, ToolCalls) :-
    findall(Text, (
        member(B, Blocks), get_dict(type, B, "text"),
        get_dict(text, B, Text)
    ), Texts),
    atomic_list_concat(Texts, Content),
    findall(TC, (
        member(B, Blocks), get_dict(type, B, "tool_use"),
        get_dict(id, B, Id), get_dict(name, B, Name),
        get_dict(input, B, Input),
        TC = _{id: Id, name: Name, arguments: Input}
    ), ToolCalls).
