# UnifyWeaver Model Registry - Public Defaults
# This file contains model definitions that are shared across installations.
# For private models (Dropbox, etc.), create config/model_registry.yaml
version: "1.0"

# =============================================================================
# Embedding Models (foundation models for encoding text)
# =============================================================================
embedding_models:

  nomic-embed-text-v1.5:
    type: embedding
    dimensions: 768
    context_length: 8192
    source:
      huggingface: "nomic-ai/nomic-embed-text-v1.5"
    features:
      - asymmetric_search  # Supports different query/document prefixes
      - matryoshka         # Can truncate to fewer dims (64, 128, 256)
      - long_context       # 8K context window
    prefixes:
      query: "search_query: "
      document: "search_document: "
    recommended_for:
      - qa_mapping
      - bookmark_filing
      - mindmap_linking
      - long_documents
    notes: |
      Default embedding model for UnifyWeaver. Matryoshka property means
      first 128 dims contain core semantic meaning - enables fast orthogonal
      codebook with only 64 rotation planes.

  all-MiniLM-L6-v2:
    type: embedding
    dimensions: 384
    context_length: 256
    source:
      huggingface: "sentence-transformers/all-MiniLM-L6-v2"
    features:
      - fast_inference
      - low_memory
      - onnx_available
    recommended_for:
      - prototyping
      - csharp_integration
      - mobile_edge
      - per_tree_clustering  # Good for many small clusters
    notes: |
      Smaller, faster model. Good for C#/ONNX integration and when
      memory is limited. Use for per-tree clustering to reduce memory.

  bge-small-en-v1.5:
    type: embedding
    dimensions: 384
    context_length: 512
    source:
      huggingface: "BAAI/bge-small-en-v1.5"
    features:
      - instruction_tuned
    prefixes:
      query: "Represent this sentence for searching relevant passages: "
    recommended_for:
      - general_search
      - retrieval

# =============================================================================
# Projection Models (trained for semantic search on specific data)
# =============================================================================
projection_models:

  pearltrees_federated_nomic:
    type: federated
    description: "All-account Pearltrees semantic search with Nomic embeddings"
    embedding_model: nomic-embed-text-v1.5
    source:
      local: "models/pearltrees_federated_nomic.pkl"
    training:
      script: "scripts/train_pearltrees_federated.py"
      data: "reports/pearltrees_targets_full_multi_account.jsonl"
      args:
        cluster_method: embedding
        cluster_criterion: effective_rank
        model: "nomic-ai/nomic-embed-text-v1.5"
    accounts:
      - s243a
      - s243a_groups
    metrics:
      clusters: 51

  pearltrees_federated_single:
    type: federated
    description: "Single-account Pearltrees model (s243a only)"
    embedding_model: nomic-embed-text-v1.5
    scope: single_account
    source:
      local: "models/pearltrees_federated_single.pkl"
    training:
      script: "scripts/train_pearltrees_federated.py"
      data: "reports/pearltrees_targets_s243a.jsonl"
      args:
        cluster_method: embedding
        cluster_criterion: effective_rank
    accounts:
      - s243a

  # ---------------------------------------------------------------------------
  # Alternative Embedding Models (MiniLM, BGE)
  # ---------------------------------------------------------------------------
  # These exist for comparison/fallback. Nomic is recommended for best quality.

  pearltrees_federated_minilm:
    type: federated
    description: "Pearltrees with MiniLM (384 dims, smaller/faster)"
    embedding_model: all-MiniLM-L6-v2
    scope: multi_account
    tags: [alternative, low_memory, onnx_compatible]
    source:
      local: "models/pearltrees_federated_minilm.pkl"
    training:
      script: "scripts/train_pearltrees_federated.py"
      data: "reports/pearltrees_targets_full_multi_account.jsonl"
      args:
        cluster_method: embedding
        model: "sentence-transformers/all-MiniLM-L6-v2"
    notes: |
      Use when memory is constrained or for C#/ONNX integration.
      Lower quality than Nomic but half the dimensions (384 vs 768).

  pearltrees_federated_bge:
    type: federated
    description: "Pearltrees with BGE embeddings (384 dims)"
    embedding_model: bge-small-en-v1.5
    scope: multi_account
    tags: [alternative]
    source:
      local: "models/pearltrees_federated_bge.pkl"
    training:
      script: "scripts/train_pearltrees_federated.py"
      data: "reports/pearltrees_targets_full_multi_account.jsonl"
      args:
        cluster_method: embedding
        model: "BAAI/bge-small-en-v1.5"
    notes: |
      Alternative to Nomic using BGE embeddings.
      Nomic generally performs better for asymmetric search.

  # ---------------------------------------------------------------------------
  # Account-Specific Models
  # ---------------------------------------------------------------------------

  pearltrees_federated_s243a:
    type: federated
    description: "s243a account Pearltrees (personal bookmarks)"
    embedding_model: nomic-embed-text-v1.5
    scope: single_account
    tags: [account_specific]
    source:
      local: "models/pearltrees_federated_s243a.pkl"
    training:
      script: "scripts/train_pearltrees_federated.py"
      data: "reports/pearltrees_targets_s243a.jsonl"
      args:
        cluster_method: embedding
        cluster_criterion: effective_rank
    accounts:
      - s243a

  pearltrees_federated_s243a_groups:
    type: federated
    description: "s243a groups/teams Pearltrees"
    embedding_model: nomic-embed-text-v1.5
    scope: single_account
    tags: [account_specific]
    source:
      local: "models/pearltrees_federated_s243a_groups.pkl"
    training:
      script: "scripts/train_pearltrees_federated.py"
      data: "reports/pearltrees_targets_s243a_groups.jsonl"
      args:
        cluster_method: embedding
        cluster_criterion: effective_rank
    accounts:
      - s243a_groups

  # ---------------------------------------------------------------------------
  # Domain-Specific Models
  # ---------------------------------------------------------------------------

  pearltrees_physics_pearls:
    type: federated
    description: "Physics domain subset"
    embedding_model: nomic-embed-text-v1.5
    scope: domain
    domain: physics
    tags: [domain_specific]
    source:
      local: "models/pearltrees_physics_pearls.pkl"
    training:
      script: "scripts/train_pearltrees_federated.py"
      data: "reports/pearltrees_targets_physics.jsonl"

  pearltrees_full_pearls:
    type: federated
    description: "Full pearls dataset (all items, not just trees)"
    embedding_model: nomic-embed-text-v1.5
    scope: multi_account
    tags: [full_dataset]
    source:
      local: "models/pearltrees_full_pearls.pkl"
    training:
      script: "scripts/train_pearltrees_federated.py"
      data: "reports/pearltrees_targets_full_pearls.jsonl"

  # ---------------------------------------------------------------------------
  # Holdout/Validation Models (for testing, not production)
  # ---------------------------------------------------------------------------

  holdout_federated_nomic:
    type: federated
    description: "Holdout validation set (Nomic) - for testing only"
    embedding_model: nomic-embed-text-v1.5
    scope: holdout
    tags: [holdout, validation, experimental]
    source:
      local: "models/holdout_federated_nomic.pkl"
    notes: "Used for model evaluation. Not for production use."

  holdout_federated_minilm:
    type: federated
    description: "Holdout validation set (MiniLM) - for testing only"
    embedding_model: all-MiniLM-L6-v2
    scope: holdout
    tags: [holdout, validation, experimental]
    source:
      local: "models/holdout_federated_minilm.pkl"
    notes: "Used for model evaluation. Not for production use."

  holdout_federated_bge:
    type: federated
    description: "Holdout validation set (BGE) - for testing only"
    embedding_model: bge-small-en-v1.5
    scope: holdout
    tags: [holdout, validation, experimental]
    source:
      local: "models/holdout_federated_bge.pkl"
    notes: "Used for model evaluation. Not for production use."

  skills_qa_federated:
    type: federated
    description: "Skills documentation Q&A retrieval"
    embedding_model: nomic-embed-text-v1.5
    source:
      local: "models/skills_qa_federated.pkl"
    training:
      script: "scripts/train_pearltrees_federated.py"
      data: ".local/data/skills_qa.jsonl"
      args:
        cluster_method: embedding

  books_qa_federated_nomic:
    type: federated
    description: "Books Q&A retrieval with Nomic embeddings"
    embedding_model: nomic-embed-text-v1.5
    source:
      local: "models/books_qa_federated_nomic.pkl"
    training:
      script: "scripts/train_pearltrees_federated.py"
      data: ".local/data/books_qa.jsonl"

# =============================================================================
# Transformer Models (fast inference via orthogonal codebook)
# =============================================================================
transformer_models:

  orthogonal_transformer_multisource:
    type: orthogonal_codebook
    description: "Fast mobile-ready projection (39x faster than weighted baseline)"
    dimensions: 768
    n_planes: 64
    source:
      local: "models/orthogonal_transformer_multisource.pt"
    training:
      script: "scripts/train_orthogonal_codebook.py"
      federated_models:
        - skills_qa_federated
        - books_qa_federated_nomic
      args:
        train_multisource: true
        codebook_method: canonical
        n_components: 64
        epochs: 50
    metrics:
      hit_at_1: 0.573
      hit_at_5: 0.826
      speed: "27K queries/sec"
    recommended_for:
      - mobile
      - edge_deployment
      - real_time
    notes: |
      Uses canonical (axis-aligned) rotation planes for perfect commutativity.
      64 planes sufficient due to Matryoshka structure of Nomic embeddings.
      See docs/design/ORTHOGONAL_CODEBOOK_DESIGN.md for theory.
