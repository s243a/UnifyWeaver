# Proposal: Python as Target Language

**Status:** Draft
**Author:** John William Creighton (@s243a)
**Co-Author:** Claude Code (Sonnet 4.5)
**Date:** 2025-11-21
**Version:** 1.0
**Related:** orchestration_architecture.md

---

## Executive Summary

Add Python as a transpilation target for UnifyWeaver, complementing existing Bash, C#, and Prolog targets. Python targets data science, ML, and scientific computing use cases. The implementation supports **dual execution modes**: in-process via Janus (preferred) and subprocess via pipes (fallback), integrating seamlessly with UnifyWeaver's orchestration layer.

**Two Implementation Approaches:**
- **Approach A (Procedural)**: Bash-style scripts with stdin/stdout - **Recommended for Phase 1**
- **Approach B (Pythonic)**: Generators and list comprehensions - Future optimization

---

## Motivation

### Why Python?

**Complements Existing Targets:**
- **Bash**: Orchestration, text processing, system integration
- **C#**: LINQ queries, enterprise, Windows environments
- **Python**: Data science, ML, scientific computing ← **Gap**
- **Prolog**: Logic programming, complex rules

**Strategic Value:**
1. **Huge User Base**: Python is #1 language for data science/ML
2. **Rich Ecosystem**: NumPy, Pandas, scikit-learn, TensorFlow
3. **Termux-Friendly**: Works excellently on Android (unlike C#)
4. **Janus Integration**: SWI-Prolog's Python bridge enables in-process execution
5. **Validates Orchestration**: 3+ targets prove orchestration architecture

### Real-World Use Cases

```prolog
% Example: Log analysis with ML
analyze_logs(LogFile, Anomalies) :-
    % Bash: Extract and filter
    extract_logs(LogFile, RawLogs),

    % Python: ML anomaly detection (via Janus)
    detect_anomalies_ml(RawLogs, Candidates),

    % C#: LINQ queries for correlation
    correlate_events(Candidates, Correlated),

    % Bash: Generate alert report
    format_alerts(Correlated, Anomalies).
```

---

## Design Goals

1. **Dual Execution Mode**: Janus (in-process) preferred, subprocess fallback
2. **Orchestration Integration**: Seamless communication with other targets
3. **Streaming Compatible**: Null-delimited JSON stdin/stdout
4. **Idiomatic Python**: Generated code looks like hand-written Python
5. **Recursive Patterns**: Support UnifyWeaver's recursion compilation
6. **Termux-Testable**: Works on Android for developer's environment

---

## Implementation Approaches

### Approach A: Procedural (Bash-Style) ← **Recommended**

**Philosophy**: Generate Python scripts similar to bash scripts - sequential, imperative, stdin/stdout

**Example Generated Code:**
```python
#!/usr/bin/env python3
"""
Generated by UnifyWeaver v0.3
Target: Python (Procedural)
Source: analyze.pl
"""

import sys
import json

def process_record(record):
    """Process a single record"""
    if record['value'] > threshold:
        return {
            'id': record['id'],
            'value': record['value'],
            'anomaly': True
        }
    return None

def main():
    threshold = 100

    # Read null-delimited JSON from stdin
    buffer = ""
    for chunk in iter(lambda: sys.stdin.read(1), ''):
        if chunk == '\0':
            if buffer:
                record = json.loads(buffer)
                result = process_record(record)
                if result:
                    print(json.dumps(result), end='\0', flush=True)
                buffer = ""
        else:
            buffer += chunk

if __name__ == '__main__':
    main()
```

**Advantages:**
- ✅ Matches existing bash pattern (familiar)
- ✅ Easy to debug (sequential execution)
- ✅ Direct stdin/stdout handling
- ✅ Simple code generation logic
- ✅ Works well with pipes

**Disadvantages:**
- ❌ Not idiomatic Python (no generators/comprehensions)
- ❌ Less efficient for large datasets
- ❌ Doesn't leverage Python's functional features

**Recommendation**: **Start here** - Proven pattern, easier implementation

---

### Approach B: Pythonic (Generator-Based)

**Philosophy**: Leverage Python's functional features - generators, comprehensions, iterators

**Example Generated Code:**
```python
#!/usr/bin/env python3
"""
Generated by UnifyWeaver v0.3
Target: Python (Pythonic)
Source: analyze.pl
"""

import sys
import json
from typing import Iterator, Dict, Any

def read_json_stream(stream) -> Iterator[Dict[str, Any]]:
    """Generator for null-delimited JSON records"""
    buffer = ""
    for chunk in iter(lambda: stream.read(1), ''):
        if chunk == '\0':
            if buffer:
                yield json.loads(buffer)
                buffer = ""
        else:
            buffer += chunk

def process_stream(records: Iterator[Dict], threshold: int) -> Iterator[Dict]:
    """Process records using generator pipeline"""
    return (
        {
            'id': r['id'],
            'value': r['value'],
            'anomaly': True
        }
        for r in records
        if r['value'] > threshold
    )

def write_json_stream(records: Iterator[Dict], stream) -> None:
    """Write records as null-delimited JSON"""
    for record in records:
        stream.write(json.dumps(record))
        stream.write('\0')
        stream.flush()

def main():
    threshold = 100

    # Functional pipeline
    records = read_json_stream(sys.stdin)
    processed = process_stream(records, threshold)
    write_json_stream(processed, sys.stdout)

if __name__ == '__main__':
    main()
```

**Advantages:**
- ✅ Idiomatic Python (generators/comprehensions)
- ✅ Memory-efficient (lazy evaluation)
- ✅ Composable (pipeline-style)
- ✅ LINQ-like (similar to C# target)

**Disadvantages:**
- ❌ More complex code generation
- ❌ Harder to debug (deferred execution)
- ❌ Requires understanding of iterators

**Recommendation**: **Phase 2** - After procedural approach proven

---

### Approach C: Hybrid (Context-Dependent)

**Philosophy**: Choose approach based on predicate characteristics

```prolog
% Simple predicate → Procedural
simple_filter(X, Y) :- X > 10, Y is X * 2.
% Generates: if x > 10: return x * 2

% Complex pipeline → Pythonic
complex_pipeline(Data, Results) :-
    map(transform, Data, T1),
    filter(predicate, T1, T2),
    fold(aggregate, T2, Results).
% Generates: return reduce(aggregate, filter(predicate, map(transform, data)))
```

**Recommendation**: **Phase 3** - After both approaches validated

---

## Dual Execution Mode

### Mode 1: Janus (In-Process) ← Preferred

```prolog
% Execute Python via Janus (same process)
execute_python_janus(PythonCode, Input, Output) :-
    % Set up Python environment
    py_call(exec(PythonCode), _),

    % Call generated Python function
    py_call(process(Input), Output).

% Example: Call Python ML library
train_model_janus(TrainingData, Model) :-
    py_call(sklearn:linear_model:'LinearRegression'(), Regressor),
    py_call(Regressor:fit(TrainingData), _),
    Model = Regressor.
```

**Advantages:**
- ✅ No subprocess overhead
- ✅ Direct data transfer (no serialization)
- ✅ Fast for small to medium datasets
- ✅ Exception propagation
- ✅ Shared memory

**Limitations:**
- ❌ Requires Janus support
- ❌ Python GIL (no parallelism)
- ❌ Version compatibility constraints

### Mode 2: Subprocess (Fallback)

```prolog
% Execute Python as subprocess with pipes
execute_python_subprocess(ScriptFile, Input, Output) :-
    process_create(
        path(python3),
        [ScriptFile],
        [stdin(pipe(In)), stdout(pipe(Out))]
    ),

    % Write null-delimited JSON
    forall(
        member(Record, Input),
        (   json_write(In, Record),
            put_char(In, '\0')
        )
    ),
    close(In),

    % Read results
    read_json_stream(Out, Output),
    close(Out).
```

**Advantages:**
- ✅ No Janus dependency
- ✅ Works everywhere
- ✅ Process isolation
- ✅ Can use multiprocessing

**Limitations:**
- ❌ Subprocess overhead
- ❌ Serialization cost
- ❌ Buffering issues
- ❌ Error handling complexity

### Intelligent Mode Selection

```prolog
% Choose execution mode based on availability and data size
choose_python_mode(DataSize, Mode) :-
    (   can_use_janus,
        DataSize < 10000  % Threshold for in-process
    ->  Mode = janus
    ;   Mode = subprocess
    ).

% Unified interface (mode selected automatically)
execute_python(Code, Input, Output) :-
    length(Input, Size),
    choose_python_mode(Size, Mode),
    execute_python_mode(Mode, Code, Input, Output).

execute_python_mode(janus, Code, Input, Output) :-
    execute_python_janus(Code, Input, Output).
execute_python_mode(subprocess, Code, Input, Output) :-
    compile_python_script(Code, ScriptFile),
    execute_python_subprocess(ScriptFile, Input, Output).
```

---

## Code Generation Patterns

### Pattern 1: Simple Predicate Translation

```prolog
% Prolog source
double(X, Y) :- Y is X * 2.

% Generated Python (Procedural)
def double(x):
    return x * 2

% Generated Python (Pythonic)
double = lambda x: x * 2
```

### Pattern 2: Recursive Predicates

```prolog
% Prolog source (linear recursion)
factorial(0, 1).
factorial(N, F) :-
    N > 0,
    N1 is N - 1,
    factorial(N1, F1),
    F is N * F1.

% Generated Python (Procedural with memoization)
_memo_factorial = {}

def factorial(n):
    if n in _memo_factorial:
        return _memo_factorial[n]

    if n == 0:
        result = 1
    elif n > 0:
        n1 = n - 1
        f1 = factorial(n1)
        result = n * f1
    else:
        return None  # Failure

    _memo_factorial[n] = result
    return result

% Generated Python (Pythonic - functools.lru_cache)
from functools import lru_cache

@lru_cache(maxsize=None)
def factorial(n):
    if n == 0:
        return 1
    elif n > 0:
        return n * factorial(n - 1)
    return None
```

### Pattern 3: List Operations

```prolog
% Prolog source
filter_positive([], []).
filter_positive([H|T], [H|R]) :- H > 0, filter_positive(T, R).
filter_positive([H|T], R) :- H =< 0, filter_positive(T, R).

% Generated Python (Procedural)
def filter_positive(lst):
    result = []
    for item in lst:
        if item > 0:
            result.append(item)
    return result

% Generated Python (Pythonic)
def filter_positive(lst):
    return [x for x in lst if x > 0]

# Or even more Pythonic:
filter_positive = lambda lst: list(filter(lambda x: x > 0, lst))
```

### Pattern 4: Streaming Pipeline

```prolog
% Prolog source
process_pipeline(Input, Output) :-
    extract_data(Input, Data),
    transform_data(Data, Transformed),
    aggregate_results(Transformed, Output).

% Generated Python (Procedural)
def process_pipeline():
    # Read from stdin
    data = extract_data(sys.stdin)

    # Transform
    transformed = transform_data(data)

    # Aggregate
    output = aggregate_results(transformed)

    # Write to stdout
    write_output(output, sys.stdout)

% Generated Python (Pythonic)
from typing import Iterable

def process_pipeline(input_stream) -> Iterable:
    return (
        aggregate_results(
            transform_data(
                extract_data(input_stream)
            )
        )
    )

# Or with pipe-like syntax (if using external library):
# from toolz import pipe
# result = pipe(
#     input_stream,
#     extract_data,
#     transform_data,
#     aggregate_results
# )
```

---

## Orchestration Integration

### Location Awareness

```prolog
% Python can execute in two locations
actual_location(python, same_process) :-
    can_use_janus, !.
actual_location(python, same_machine).

% Prefer Python for data science tasks
task_target_preference(machine_learning(_), python, 10).
task_target_preference(data_analysis(_), python, 9).
task_target_preference(numerical_computation(_), python, 9).
```

### Communication with Other Targets

```prolog
% Bash → Python → C# pipeline
multi_target_pipeline(CSVFile, Report) :-
    % Bash: Extract CSV
    bash_target:compile(extract_csv(CSVFile), BashScript),

    % Python: ML analysis (via Janus if available)
    python_target:compile(analyze_ml, PythonCode),
    choose_python_mode(_, Mode),

    % C#: LINQ aggregation
    csharp_target:compile(aggregate_linq, CSharpCode),

    % Execute coordinated pipeline
    execute_coordinated([
        stage(bash, BashScript, file(CSVFile)),
        stage(python, PythonCode, stdin, Mode),
        stage(csharp, CSharpCode, stdin)
    ], Report).
```

---

## Implementation Plan

### Phase 1: Procedural Python Target (8-12 hours)

**Goal**: Generate basic Python scripts (bash-style)

**Tasks:**
1. Create `src/unifyweaver/targets/python_target.pl` module
2. Implement procedural code generation
3. Generate stdin/stdout handling code
4. Support null-delimited JSON streaming
5. Test: Simple predicates → Python scripts

**Deliverable**: Can generate Python scripts that integrate with pipes

---

### Phase 2: Janus Integration (6-8 hours)

**Goal**: Support in-process Python execution via Janus

**Tasks:**
1. Implement Janus detection (`can_use_janus`)
2. Create `execute_python_janus/3` predicate
3. Add mode selection logic
4. Test: Same code works via Janus and subprocess
5. Benchmark: Janus vs subprocess performance

**Deliverable**: Automatic mode selection working

---

### Phase 3: Recursion Patterns (8-10 hours)

**Goal**: Support UnifyWeaver's recursion compilation for Python

**Tasks:**
1. Linear recursion → Python with memoization
2. Mutual recursion → Two Python functions
3. Tree recursion → Nested data structures
4. Fold pattern → functools.reduce
5. Test: Factorial, Fibonacci, Tree sum

**Deliverable**: Recursive predicates compile to Python

---

### Phase 4: Pythonic Code Generation (10-12 hours)

**Goal**: Generate idiomatic Python with generators/comprehensions

**Tasks:**
1. Detect list operations → comprehensions
2. Detect pipelines → generators
3. Use type hints (Python 3.10+)
4. Optimize with `functools.lru_cache`
5. Test: Compare procedural vs Pythonic output

**Deliverable**: Can generate Pythonic code for suitable predicates

---

### Phase 5: Orchestration Integration (4-6 hours)

**Goal**: Seamless integration with orchestration layer

**Tasks:**
1. Register Python target with orchestration
2. Implement location predicates
3. Test multi-target pipelines (Bash → Python → C#)
4. Document orchestration examples

**Deliverable**: Python works in orchestrated pipelines

---

### Phase 6: Testing & Documentation (6-8 hours)

**Tasks:**
1. Comprehensive test suite
2. Janus vs subprocess benchmarks
3. Documentation: Python Target Guide
4. Examples: ML pipeline, data analysis
5. Termux testing (user's environment)

**Deliverable**: Tested, documented Python target

---

**Total Estimated Effort:** 42-56 hours (across 6 phases)

---

## Comparison with Other Targets

| Feature | Bash | C# | Python | Prolog |
|---------|------|----|----|--------|
| **Execution** | Subprocess | Query runtime / Codegen | Janus / Subprocess | Script |
| **Recursion** | Limited | LINQ / Templates | Memoization / Generators | Native |
| **Streaming** | Pipes | JSON | JSON / Generators | Terms |
| **ML/Data Science** | ❌ | ✅ (ML.NET) | ✅✅ (Best) | ❌ |
| **Termux Support** | ✅✅ | ❌ | ✅✅ | ✅✅ |
| **Performance** | Fast (for text) | Fast (compiled) | Medium (interpreted) | Medium |
| **Orchestration** | ✅✅ (Primary) | ✅ (Secondary) | ✅✅ (Janus) | ✅ (Script) |

---

## Open Questions

1. **Python Version**: Target Python 3.8+, 3.10+, or 3.12+?
2. **Type Hints**: Always generate, optional, or never?
3. **Error Handling**: Try/except or return None on failure?
4. **Dependencies**: Allow import of external libraries?
5. **Virtual Environments**: How to handle venv/conda?
6. **Async/Await**: Support async Python for I/O-bound tasks?
7. **Multiprocessing**: Allow parallel execution?
8. **Pandas Integration**: Special support for DataFrames?

---

## Success Criteria

**Phase 1 (Procedural):**
- ✅ Can generate Python scripts from simple predicates
- ✅ Stdin/stdout handling works
- ✅ Null-delimited JSON streaming functional

**Phase 2 (Janus):**
- ✅ Janus mode works for in-process execution
- ✅ Automatic fallback to subprocess
- ✅ Performance improvement measurable

**Phase 3 (Recursion):**
- ✅ Factorial compiles to Python with memoization
- ✅ Fibonacci works (multi-call linear recursion)
- ✅ All recursion patterns supported

**Phase 5 (Orchestration):**
- ✅ Bash → Python → C# pipeline works
- ✅ Location-aware execution
- ✅ Seamless target switching

**Phase 6 (Termux):**
- ✅ User can test on Termux/Android
- ✅ All examples work on Termux
- ✅ Documentation tested on platform

---

## Related Documents

- `docs/proposals/orchestration_architecture.md` - Orchestration design
- `docs/proposals/prolog_as_target_language.md` - Prolog target
- `docs/proposals/target_language_comparison.md` - Target comparison
- `tests/core/test_csharp_janus.pl` - Existing Janus usage

---

**Status:** Draft - Awaiting review and approval
**Recommendation:** Approve for implementation starting with Phase 1 (Procedural)
**Priority:** High - Validates orchestration architecture, Termux-testable
