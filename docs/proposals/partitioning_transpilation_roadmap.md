# Partitioning Transpilation - Complete Roadmap

**Date:** 2025-10-27
**Status:** ‚úÖ Phase 1 Foundation Complete | üöß Phases 2-4 Planned
**Branch:** `feat/prolog-as-target`
**Implementation:** `src/unifyweaver/targets/prolog_target.pl` (~400 lines, tests passing)

---

## Executive Summary

UnifyWeaver currently implements partitioning/parallel execution as **Prolog runtime code**. This document provides a complete roadmap for three transpilation approaches:

1. **Prolog as Target** - Generate standalone Prolog scripts (easiest, ~25 hours)
2. **Prolog-as-a-Service in Bash** - Bash calls Prolog for complex operations (~35 hours)
3. **Pure Bash Partitioning** - Full bash transpilation with file-based partitions (~50 hours)

**Recommendation**: Implement in this order (easiest to hardest), with each phase building on previous work.

---

## Table of Contents

1. [Current State Analysis](#current-state)
2. [Three Approaches Compared](#comparison)
3. [Approach 1: Prolog as Target](#approach-1)
4. [Approach 2: Prolog-as-a-Service](#approach-2)
5. [Approach 3: Pure Bash Partitioning](#approach-3)
6. [Unified Implementation Roadmap](#roadmap)
7. [Future Enhancements](#future)

---

## Current State Analysis {#current-state}

### What We Have

**Implemented (Prolog Runtime):**
- ‚úÖ `src/unifyweaver/core/partitioner.pl` - Plugin interface
- ‚úÖ `src/unifyweaver/core/partitioners/fixed_size.pl` - Row/byte partitioning
- ‚úÖ `src/unifyweaver/core/partitioners/hash_based.pl` - Hash partitioning
- ‚úÖ `src/unifyweaver/core/partitioners/key_based.pl` - GROUP BY partitioning
- ‚úÖ `src/unifyweaver/core/parallel_backend.pl` - Backend interface
- ‚úÖ `src/unifyweaver/core/backends/gnu_parallel.pl` - GNU Parallel integration

**Status**: Works perfectly as Prolog runtime, but NOT transpiled.

### The Problem

```prolog
% Current: User writes Prolog that RUNS in Prolog
main :-
    partitioner_init(fixed_size(rows(100)), [], H),
    partitioner_partition(H, Data, Partitions),
    backend_execute(BHandle, Partitions, 'script.sh', Results).
```

**Issue**: This code executes in SWI-Prolog, doesn't transpile to bash/PowerShell.

### The Goal

```prolog
% User writes:
process_data(File, Results) :-
    partition_csv(File, fixed_size(rows(100)), Partitions),
    parallel_map(Partitions, double/1, Results).

% UnifyWeaver generates: (one of three approaches)
# 1. Standalone Prolog script (#!/usr/bin/env swipl)
# 2. Bash script that calls Prolog for partitioning
# 3. Pure bash script with file-based partitioning
```

---

## Three Approaches Compared {#comparison}

| Aspect | Prolog Target | Prolog-as-Service | Pure Bash |
|--------|---------------|-------------------|-----------|
| **Complexity** | Low | Medium | High |
| **Effort** | ~25 hours | ~35 hours | ~50 hours |
| **Dependencies** | SWI-Prolog | SWI-Prolog + bash | bash + GNU tools |
| **Portability** | Requires Prolog | Requires Prolog | Runs everywhere |
| **Performance** | Fast (native) | Mixed (IPC overhead) | Fast (native) |
| **Data Structures** | Excellent | Excellent | Limited |
| **Suitable For** | Complex logic | Hybrid (bash + Prolog) | Simple pipelines |
| **Debugging** | Prolog tools | Mixed | Bash tools |

### Recommended Order

1. **Phase 1**: Prolog as Target (easiest, fast to implement)
2. **Phase 2**: Prolog-as-a-Service (bridge to bash)
3. **Phase 3**: Pure Bash (full portability)

Each phase builds on previous work, allowing incremental delivery.

---

## Approach 1: Prolog as Target {#approach-1}

### Overview

**Concept**: Transpile user Prolog ‚Üí Standalone executable Prolog script

**Output**: `#!/usr/bin/env swipl` script that includes runtime library

### Generated Code Structure

```prolog
#!/usr/bin/env swipl
% Generated by UnifyWeaver v0.0.3
% Source: user_code.pl

% === Runtime Library Setup ===
:- use_module(library(unifyweaver/partitioner)).
:- use_module(library(unifyweaver/parallel_backend)).

% === User Code (Transpiled) ===
process_data(File, Results) :-
    % Partitioning
    partitioner_init(fixed_size(rows(100)), [], PHandle),
    read_csv(File, Data),
    partitioner_partition(PHandle, Data, Partitions),
    partitioner_cleanup(PHandle),

    % Parallel execution
    backend_init(gnu_parallel(workers(4)), BHandle),
    backend_execute(BHandle, Partitions, 'worker.sh', Results),
    backend_cleanup(BHandle).

% === Entry Point ===
main :-
    current_prolog_flag(argv, [File]),
    process_data(File, Results),
    write_results(Results).

:- initialization(main, main).
```

### Implementation Strategy

#### Step 1: Code Analysis & Dependency Detection

```prolog
% Analyze user predicates to determine what to include
analyze_user_code(UserPredicates, Analysis) :-
    Analysis = analysis{
        uses_partitioning: UsesPartitioning,
        partitioning_strategies: Strategies,
        uses_parallel: UsesParallel,
        parallel_backends: Backends,
        required_modules: Modules
    },
    detect_features(UserPredicates, Analysis).

% Example detection
detect_features(Predicates, Analysis) :-
    findall(partition_call(Pred, Strategy), (
        member(Pred, Predicates),
        clause(Pred, Body),
        contains_goal(Body, partition_data(_, Strategy, _))
    ), PartitionCalls),
    % ... extract strategies, backends, etc.
```

#### Step 2: Code Generation (Three Methods)

**Method A: Verbatim Copy (Simple Predicates)**

```prolog
% For predicates with no configuration
copy_predicate_verbatim(Pred/Arity, Stream) :-
    functor(Head, Pred, Arity),
    clause(Head, Body),
    portray_clause(Stream, (Head :- Body)).
```

**Method B: Template Substitution (Configured Predicates)**

```prolog
% For predicates with configuration substitution
generate_from_template(partition_data(File, Config, Partitions), Code) :-
    % Template with config substituted
    Code = (
        partitioner_init(Config, [], Handle),
        read_data(File, Data),
        partitioner_partition(Handle, Data, Partitions),
        partitioner_cleanup(Handle)
    ).
```

**Method C: Inline Runtime Library (Optional)**

```prolog
% Instead of :- use_module(...), inline the code
inline_runtime_library(Module, Stream) :-
    module_source_file(Module, SourceFile),
    read_file_to_codes(SourceFile, Codes),
    format(Stream, '% === Inlined: ~w ===~n', [Module]),
    format(Stream, '~s~n', [Codes]).
```

#### Step 3: Module Search Path Setup

**Option A: Environment Variable**
```prolog
% In generated script
:- (   getenv('UNIFYWEAVER_HOME', Home)
   ->  asserta(file_search_path(unifyweaver, Home))
   ;   % Fallback: assume installed as pack
       true
   ).
```

**Option B: SWI-Prolog Pack**
```prolog
% User installs once:
$ swipl-pack install unifyweaver

% Generated script just uses library()
:- use_module(library(unifyweaver/partitioner)).
```

**Option C: Relative Path**
```prolog
% Calculate path relative to script location
:- prolog_load_context(directory, ScriptDir),
   atom_concat(ScriptDir, '/../runtime', RuntimeDir),
   asserta(file_search_path(unifyweaver, RuntimeDir)).
```

### Effort Breakdown

| Task | Hours | Difficulty |
|------|-------|------------|
| Code analysis & dependency detection | 4 | Medium |
| Verbatim copying | 3 | Easy |
| Template substitution | 5 | Medium |
| Module search path setup | 3 | Easy |
| Entry point generation | 2 | Easy |
| Runtime library packaging | 4 | Medium |
| Integration with compiler | 4 | Medium |
| Testing & documentation | 4 | Easy |
| **Total** | **29 hours** | **Low-Medium** |

### Advantages

- ‚úÖ Fast to implement
- ‚úÖ Leverages existing Prolog code
- ‚úÖ Full power of Prolog (complex data structures)
- ‚úÖ Easy to debug (Prolog tools)

### Disadvantages

- ‚ùå Requires SWI-Prolog on target
- ‚ùå Not "pure" transpilation (Prolog ‚Üí Prolog)
- ‚ùå Generated scripts less portable

---

## Approach 2: Prolog-as-a-Service in Bash {#approach-2}

### Overview

**Concept**: Generate bash scripts that call Prolog for complex operations

**Architecture**: Bash orchestration + Prolog computation

### Generated Code Structure

```bash
#!/bin/bash
# Generated by UnifyWeaver v0.0.4
# Source: user_code.pl

# === Helper: Call Prolog for partitioning ===
partition_data() {
    local input_file="$1"
    local strategy="$2"
    local partition_dir="$3"

    swipl -q -g "
        use_module(library(unifyweaver/partitioner)),
        read_csv('$input_file', Data),
        partitioner_init($strategy, [], H),
        partitioner_partition(H, Data, Partitions),
        write_partitions_to_files(Partitions, '$partition_dir'),
        halt.
    " -t halt
}

# === User Code (Transpiled to Bash) ===
process_data() {
    local input_file="$1"

    # Partition using Prolog
    partition_data "$input_file" "fixed_size(rows(100))" "/tmp/partitions_$$"

    # Process partitions in parallel (pure bash + GNU Parallel)
    parallel --jobs 4 "bash worker.sh < {}" ::: /tmp/partitions_$$/*.txt

    # Collect results
    cat /tmp/partitions_$$/output_*.txt

    # Cleanup
    rm -rf "/tmp/partitions_$$"
}

# === Entry Point ===
process_data "$1"
```

### Implementation Strategy

#### Step 1: Bash-Prolog IPC Protocol

**File-Based Communication:**

```bash
# Bash writes input
echo "$data" > /tmp/input_$$.pl

# Bash calls Prolog
swipl -q -g "
    read_file_to_terms('/tmp/input_$$.pl', Data),
    partition_data(Data, Strategy, Partitions),
    write_partitions('/tmp/output_$$.pl', Partitions),
    halt.
" -t halt

# Bash reads output
source /tmp/output_$$.pl  # Bash can source Prolog-like syntax
```

**Prolog Output Format (Bash-Compatible):**

```prolog
% Prolog generates bash-compatible output
write_partitions_as_bash(Partitions, File) :-
    open(File, write, Stream),
    forall(member(partition(ID, Data), Partitions), (
        format(Stream, 'PARTITION_~w=(~n', [ID]),
        forall(member(Item, Data), (
            format(Stream, '  "~w"~n', [Item])
        )),
        format(Stream, ')~n', [])
    )),
    close(Stream).

% Generates:
% PARTITION_0=(
%   "item1"
%   "item2"
% )
% PARTITION_1=(
%   "item3"
%   "item4"
% )
```

#### Step 2: File-Based Partition Storage

**Directory Structure:**
```
/tmp/unifyweaver_partitions_12345/
‚îú‚îÄ‚îÄ metadata.sh           # Partition count, strategy info
‚îú‚îÄ‚îÄ partition_0.txt       # Partition 0 data
‚îú‚îÄ‚îÄ partition_1.txt       # Partition 1 data
‚îú‚îÄ‚îÄ partition_2.txt       # Partition 2 data
‚îî‚îÄ‚îÄ partition_3.txt       # Partition 3 data
```

**Metadata File (Bash Source-able):**
```bash
# metadata.sh
PARTITION_COUNT=4
PARTITION_STRATEGY="fixed_size(rows(100))"
PARTITION_FILES=(
    "partition_0.txt"
    "partition_1.txt"
    "partition_2.txt"
    "partition_3.txt"
)
```

**Prolog Writes Files:**
```prolog
write_partitions_to_files(Partitions, BaseDir) :-
    make_directory(BaseDir),

    % Write metadata
    format(atom(MetaFile), '~w/metadata.sh', [BaseDir]),
    write_metadata(MetaFile, Partitions),

    % Write partition files
    forall(member(partition(ID, Data), Partitions), (
        format(atom(PartFile), '~w/partition_~w.txt', [BaseDir, ID]),
        write_partition_file(PartFile, Data)
    )).
```

#### Step 3: Bash Orchestration

**Generated Bash Wrapper:**

```bash
#!/bin/bash

partition_and_process() {
    local input="$1"
    local strategy="$2"
    local worker_script="$3"

    # Create temp directory
    local partition_dir="/tmp/unifyweaver_$$"
    mkdir -p "$partition_dir"

    # Call Prolog to partition
    swipl -q <<EOF
:- use_module(library(unifyweaver/partitioner)).
:- initialization(main, main).

main :-
    read_csv('$input', Data),
    partitioner_init($strategy, [], H),
    partitioner_partition(H, Data, Partitions),
    write_partitions_to_files(Partitions, '$partition_dir'),
    halt(0).
EOF

    # Load metadata
    source "$partition_dir/metadata.sh"

    # Process partitions in parallel
    cd "$partition_dir"
    parallel --jobs 4 "bash $worker_script < {}" ::: "${PARTITION_FILES[@]}"

    # Cleanup
    cd /
    rm -rf "$partition_dir"
}
```

### Effort Breakdown

| Task | Hours | Difficulty |
|------|-------|------------|
| Bash-Prolog IPC protocol | 6 | Medium |
| File-based partition storage | 5 | Medium |
| Prolog partition writer | 4 | Easy |
| Bash partition reader | 3 | Easy |
| Bash orchestration templates | 5 | Medium |
| GNU Parallel integration | 4 | Easy |
| Error handling & cleanup | 4 | Medium |
| Testing & documentation | 4 | Medium |
| **Total** | **35 hours** | **Medium** |

### Advantages

- ‚úÖ Bash orchestration (familiar to scripters)
- ‚úÖ Prolog for complex logic (partitioning algorithms)
- ‚úÖ Can mix bash and Prolog seamlessly
- ‚úÖ Still requires Prolog, but more bash-like

### Disadvantages

- ‚ùå IPC overhead (file I/O)
- ‚ùå More complex (two languages)
- ‚ùå Still requires SWI-Prolog

---

## Approach 3: Pure Bash Partitioning {#approach-3}

### Overview

**Concept**: Full bash transpilation using file-based partitioning

**No Prolog Dependency**: Generated scripts are pure bash

### File-Based Partitioning Architecture

#### Core Concept

**Partition = File on Disk**

```
/tmp/unifyweaver_partitions_12345/
‚îú‚îÄ‚îÄ metadata.json         # Partition metadata (count, strategy, sizes)
‚îú‚îÄ‚îÄ step_0_partition_0    # Step 0 (initial partition), bin 0
‚îú‚îÄ‚îÄ step_0_partition_1    # Step 0, bin 1
‚îú‚îÄ‚îÄ step_0_partition_2    # Step 0, bin 2
‚îú‚îÄ‚îÄ step_0_partition_3    # Step 0, bin 3
‚îú‚îÄ‚îÄ step_1_partition_0    # Step 1 (re-partition), bin 0
‚îî‚îÄ‚îÄ step_1_partition_1    # Step 1, bin 1
```

**Naming Convention:**
- `step_N` - Partition stage number (allows multi-step partitioning)
- `partition_M` - Partition bin number

#### Strategy 1: Fixed-Size Partitioning (Rows)

**Implementation: Pure Bash**

```bash
#!/bin/bash

# Partition by rows (100 rows per partition)
partition_by_rows() {
    local input_file="$1"
    local rows_per_partition="$2"
    local output_dir="$3"

    mkdir -p "$output_dir"

    # Use split command (part of coreutils)
    split -l "$rows_per_partition" \
          -d \
          --additional-suffix=.txt \
          "$input_file" \
          "$output_dir/step_0_partition_"

    # Generate metadata
    local partition_count=$(ls "$output_dir"/step_0_partition_* | wc -l)
    cat > "$output_dir/metadata.json" <<EOF
{
  "strategy": "fixed_size",
  "mode": "rows",
  "rows_per_partition": $rows_per_partition,
  "partition_count": $partition_count,
  "step": 0
}
EOF
}

# Usage
partition_by_rows "input.csv" 100 "/tmp/partitions_$$"
```

**Advantages:**
- ‚úÖ Uses standard `split` command
- ‚úÖ Very fast (native C implementation)
- ‚úÖ No dependencies beyond coreutils

#### Strategy 2: Fixed-Size Partitioning (Bytes)

**Implementation: Pure Bash**

```bash
# Partition by bytes (1MB per partition)
partition_by_bytes() {
    local input_file="$1"
    local bytes_per_partition="$2"  # e.g., 1048576 for 1MB
    local output_dir="$3"

    mkdir -p "$output_dir"

    # Use split with byte mode
    split -b "$bytes_per_partition" \
          -d \
          --additional-suffix=.txt \
          "$input_file" \
          "$output_dir/step_0_partition_"

    # Metadata...
}
```

#### Strategy 3: Hash-Based Partitioning

**Challenge**: Bash doesn't have built-in hash functions

**Solution A: Use `cksum` (POSIX)**

```bash
# Hash a key using cksum
hash_key() {
    local key="$1"
    local num_partitions="$2"

    # cksum outputs: checksum size filename
    # We take checksum % num_partitions
    local hash=$(echo -n "$key" | cksum | cut -d' ' -f1)
    echo $(( hash % num_partitions ))
}

# Partition CSV by hashing column 1
partition_by_hash() {
    local input_file="$1"
    local key_column="$2"
    local num_partitions="$3"
    local output_dir="$4"

    mkdir -p "$output_dir"

    # Create partition files
    for i in $(seq 0 $((num_partitions - 1))); do
        touch "$output_dir/step_0_partition_$i"
    done

    # Read CSV and assign to partitions
    while IFS=',' read -r line; do
        # Extract key column (AWK)
        key=$(echo "$line" | awk -F',' "{print \$$key_column}")

        # Hash key to determine partition
        partition_id=$(hash_key "$key" "$num_partitions")

        # Append to partition file
        echo "$line" >> "$output_dir/step_0_partition_$partition_id"
    done < "$input_file"
}
```

**Solution B: Use AWK (More Efficient)**

```bash
# Hash-based partitioning using AWK
partition_by_hash_awk() {
    local input_file="$1"
    local key_column="$2"
    local num_partitions="$3"
    local output_dir="$4"

    mkdir -p "$output_dir"

    # AWK script for hash partitioning
    awk -F',' -v key_col="$key_column" \
              -v num_parts="$num_partitions" \
              -v out_dir="$output_dir" '
    function hash(key, mod) {
        # Simple string hash (djb2 algorithm)
        h = 5381
        for (i = 1; i <= length(key); i++) {
            c = substr(key, i, 1)
            h = ((h * 33) + ord(c)) % 2147483647
        }
        return h % mod
    }

    function ord(c) {
        # Character to ASCII (GNU AWK extension)
        return sprintf("%c", c)
    }

    {
        key = $key_col
        partition = hash(key, num_parts)
        file = out_dir "/step_0_partition_" partition
        print $0 >> file
    }
    ' "$input_file"
}
```

#### Strategy 4: Key-Based Partitioning (GROUP BY)

**Challenge**: Need associative grouping

**Solution: AWK with Associative Arrays**

```bash
# Key-based partitioning (one file per unique key)
partition_by_key() {
    local input_file="$1"
    local key_column="$2"
    local output_dir="$3"

    mkdir -p "$output_dir"

    # First pass: collect unique keys
    local keys=$(awk -F',' -v col="$key_column" '{print $col}' "$input_file" | sort -u)

    # Create partition for each key
    local partition_id=0
    declare -A key_to_partition

    while IFS= read -r key; do
        key_to_partition["$key"]=$partition_id
        touch "$output_dir/step_0_partition_${partition_id}"
        echo "$key" > "$output_dir/step_0_partition_${partition_id}.key"
        partition_id=$((partition_id + 1))
    done <<< "$keys"

    # Second pass: distribute rows
    awk -F',' -v col="$key_column" '
    NR == FNR {
        key_map[$1] = FNR - 1
        next
    }
    {
        key = $col
        if (key in key_map) {
            file = output_dir "/step_0_partition_" key_map[key]
            print $0 >> file
        }
    }
    ' <(echo "$keys") "$input_file"
}
```

#### Multi-Step Partitioning

**Concept**: Partition, process, re-partition

```bash
# Example: Partition ‚Üí Map ‚Üí Re-partition ‚Üí Reduce
pipeline() {
    local input="$1"

    # Step 0: Initial partition by hash
    partition_by_hash "$input" 1 4 "/tmp/stage1_$$"

    # Step 1: Map phase (process each partition)
    for file in /tmp/stage1_$$/step_0_partition_*; do
        bash map_script.sh < "$file" > "${file}.mapped"
    done

    # Step 2: Re-partition by key
    cat /tmp/stage1_$$/step_0_partition_*.mapped > /tmp/mapped_$$.txt
    partition_by_key "/tmp/mapped_$$.txt" 1 "/tmp/stage2_$$"

    # Step 3: Reduce phase
    for file in /tmp/stage2_$$/step_0_partition_*; do
        bash reduce_script.sh < "$file"
    done
}
```

### Optimization: Redis/Compression/Caching

#### Redis-Backed Partitions (Future Enhancement)

```bash
# Store partitions in Redis instead of files
redis_partition_write() {
    local partition_id="$1"
    local data="$2"
    local session_id="$3"

    # Compress data
    local compressed=$(echo "$data" | gzip -c | base64)

    # Store in Redis with TTL
    redis-cli -x SET "partition:$session_id:$partition_id" <<< "$compressed"
    redis-cli EXPIRE "partition:$session_id:$partition_id" 3600
}

redis_partition_read() {
    local partition_id="$1"
    local session_id="$2"

    # Fetch from Redis
    redis-cli GET "partition:$session_id:$partition_id" | \
        base64 -d | \
        gunzip
}
```

#### Compression for Large Partitions

```bash
# Compress partitions on write
partition_with_compression() {
    local input="$1"
    local output_dir="$2"

    # Partition as usual
    partition_by_rows "$input" 1000 "$output_dir"

    # Compress each partition with zstd (fast compression)
    for file in "$output_dir"/step_0_partition_*; do
        zstd -q --fast=1 "$file" -o "$file.zst"
        rm "$file"  # Remove uncompressed
    done
}

# Process compressed partitions
process_compressed() {
    local partition_file="$1"

    # Decompress on-the-fly
    zstdcat "$partition_file" | bash worker.sh
}
```

#### Disk Caching with Memory Mapping

```bash
# Use memory-mapped files for large partitions
mmap_partition() {
    local partition_file="$1"

    # On Linux, use mmap via /proc
    # Kernel handles paging automatically
    dd if="$partition_file" bs=4M status=none | process_data
}
```

### Effort Breakdown

| Task | Hours | Difficulty |
|------|-------|------------|
| Fixed-size partitioning (bash) | 4 | Easy |
| Hash-based partitioning (AWK) | 8 | Medium |
| Key-based partitioning (AWK) | 8 | Medium |
| File metadata generation | 3 | Easy |
| Multi-step partition pipeline | 6 | Medium |
| GNU Parallel integration | 4 | Easy |
| Compression support (zstd) | 4 | Easy |
| Redis backend (optional) | 8 | Hard |
| Testing & benchmarks | 8 | Medium |
| Documentation | 5 | Easy |
| **Total (without Redis)** | **50 hours** | **Medium-High** |
| **Total (with Redis)** | **58 hours** | **High** |

### Advantages

- ‚úÖ Zero Prolog dependency
- ‚úÖ Runs on any Unix system (bash + coreutils)
- ‚úÖ File-based = can resume failed jobs
- ‚úÖ Compression/Redis = handles large data
- ‚úÖ True transpilation (Prolog ‚Üí bash)

### Disadvantages

- ‚ùå More code to generate
- ‚ùå Bash limited data structures (arrays, not lists/terms)
- ‚ùå AWK required for complex partitioning
- ‚ùå File I/O overhead for small datasets

---

## Unified Implementation Roadmap {#roadmap}

### Phase 1: Prolog as Target (v0.0.3)

**Goal**: Generate standalone Prolog scripts

**Timeline**: 3-4 weeks

**Deliverables:**
1. Prolog target generator
2. Runtime library as SWI-Prolog pack
3. Template-based code generation
4. Tests and documentation

**Success Criteria:**
- ‚úÖ User writes Prolog, generates executable `.pl` script
- ‚úÖ Script includes runtime library
- ‚úÖ Works for all three partitioning strategies

**Branch**: `feat/prolog-as-target`

---

### Phase 2: Prolog-as-a-Service (v0.0.4)

**Goal**: Bash scripts call Prolog for partitioning

**Timeline**: 4-5 weeks

**Deliverables:**
1. Bash-Prolog IPC protocol
2. File-based partition storage
3. Bash orchestration templates
4. Mixed bash/Prolog code generation

**Success Criteria:**
- ‚úÖ User writes Prolog, generates bash script
- ‚úÖ Bash calls Prolog for partitioning
- ‚úÖ Bash handles parallel execution
- ‚úÖ Works on any Unix system with SWI-Prolog

**Branch**: `feat/prolog-as-service`

---

### Phase 3: Pure Bash Partitioning (v0.0.5)

**Goal**: Full bash transpilation, zero Prolog dependency

**Timeline**: 6-7 weeks

**Deliverables:**
1. Bash fixed-size partitioning (split)
2. Bash hash partitioning (AWK + cksum)
3. Bash key-based partitioning (AWK)
4. File-based partition metadata
5. Multi-step partition pipelines

**Success Criteria:**
- ‚úÖ User writes Prolog, generates pure bash
- ‚úÖ No Prolog dependency at runtime
- ‚úÖ All three strategies implemented
- ‚úÖ Performance comparable to Prolog version

**Branch**: `feat/bash-partitioning`

---

### Phase 4: Optimizations (v0.1.0)

**Goal**: Performance enhancements for large datasets

**Timeline**: 4-5 weeks

**Deliverables:**
1. Compression support (zstd)
2. Redis-backed partitions (optional)
3. Memory-mapped file support
4. Benchmark suite
5. Performance tuning guide

**Success Criteria:**
- ‚úÖ Handles 10GB+ datasets efficiently
- ‚úÖ Compression reduces I/O by 5-10√ó
- ‚úÖ Redis option for distributed systems
- ‚úÖ Performance benchmarks documented

**Branch**: `feat/partition-optimizations`

---

## Detailed TODO List

### Phase 1 TODOs: Prolog as Target

**Week 1: Foundation**
- [ ] Create `src/unifyweaver/targets/prolog_target.pl`
- [ ] Implement user code analysis (detect features used)
- [ ] Implement dependency detection (which modules needed)
- [ ] Create basic script template (shebang, initialization, main)

**Week 2: Code Generation**
- [ ] Implement verbatim predicate copying
- [ ] Implement template-based generation
- [ ] Implement configuration substitution
- [ ] Handle module qualifications

**Week 3: Runtime Library**
- [ ] Organize runtime modules in `runtime/` directory
- [ ] Create SWI-Prolog pack metadata (`pack.pl`)
- [ ] Implement module search path setup
- [ ] Test pack installation: `swipl-pack install`

**Week 4: Integration & Testing**
- [ ] Integrate with main compiler pipeline
- [ ] Add `--target prolog` CLI flag
- [ ] Create test suite for Prolog target
- [ ] Write documentation: "Prolog as Target"
- [ ] Update examples to show Prolog target

---

### Phase 2 TODOs: Prolog-as-a-Service

**Week 1: IPC Protocol**
- [ ] Design file-based Prolog‚ÜîBash communication
- [ ] Implement Prolog partition file writer
- [ ] Implement bash partition file reader
- [ ] Create bash-compatible metadata format

**Week 2: Partition Storage**
- [ ] Design partition directory structure
- [ ] Implement partition metadata generation
- [ ] Create partition file naming convention
- [ ] Handle cleanup and temp file management

**Week 3: Bash Templates**
- [ ] Create bash wrapper templates
- [ ] Implement Prolog invocation from bash
- [ ] Integrate GNU Parallel execution
- [ ] Handle error propagation

**Week 4: Code Generation**
- [ ] Generate hybrid bash/Prolog scripts
- [ ] Substitute user configuration
- [ ] Add entry point and argument parsing
- [ ] Test mixed-mode execution

**Week 5: Testing & Docs**
- [ ] Test all partitioning strategies
- [ ] Benchmark vs pure Prolog
- [ ] Write documentation
- [ ] Create examples

---

### Phase 3 TODOs: Pure Bash Partitioning

**Week 1-2: Fixed-Size**
- [ ] Implement bash row partitioning (split)
- [ ] Implement bash byte partitioning
- [ ] Generate partition metadata
- [ ] Test with GNU Parallel

**Week 3-4: Hash-Based**
- [ ] Implement hash function in bash (cksum)
- [ ] Implement AWK-based hash partitioning
- [ ] Test deterministic assignment
- [ ] Benchmark hash distribution

**Week 5-6: Key-Based**
- [ ] Implement AWK-based grouping
- [ ] Handle arbitrary number of keys
- [ ] Generate key metadata
- [ ] Test with GROUP BY examples

**Week 7: Integration**
- [ ] Add `--target bash` mode
- [ ] Generate pure bash scripts
- [ ] Test all strategies
- [ ] Write documentation

---

### Phase 4 TODOs: Optimizations

**Week 1: Compression**
- [ ] Integrate zstd compression
- [ ] Benchmark compression ratios
- [ ] Test with large datasets (10GB+)
- [ ] Document compression benefits

**Week 2-3: Redis Backend**
- [ ] Design Redis key schema
- [ ] Implement Redis partition writer
- [ ] Implement Redis partition reader
- [ ] Test distributed access

**Week 4: Memory Mapping**
- [ ] Research Linux mmap for partitions
- [ ] Implement memory-mapped reads
- [ ] Benchmark vs file I/O
- [ ] Document use cases

**Week 5: Benchmarking**
- [ ] Create benchmark suite
- [ ] Compare all three approaches
- [ ] Test on various dataset sizes
- [ ] Write performance guide

---

## Future Enhancements {#future}

### 1. Range-Based Partitioning

**Use Case**: Time-series data, sorted data

```bash
# Partition by timestamp ranges
partition_by_range() {
    local input="$1"
    local column="$2"
    local ranges="$3"  # "2024-01-01:2024-01-31,2024-02-01:2024-02-28"

    # AWK script to assign to range bins
    # ...
}
```

**Effort**: 8-10 hours

---

### 2. Histogram-Based Partitioning

**Use Case**: Balanced partitions for skewed data

```bash
# Pre-analyze data distribution, create balanced partitions
partition_by_histogram() {
    # Pass 1: Build histogram
    # Pass 2: Determine partition boundaries
    # Pass 3: Assign to partitions
}
```

**Effort**: 12-15 hours

---

### 3. Streaming Partitioning (Event-Driven)

**Use Case**: Infinite streams, real-time processing

```prolog
% Partition on-the-fly as data arrives
process_stream(InputStream, Partitioner, OutputStream) :-
    % Assign each item as it arrives
    % Trigger processing when partition full
    % Stream results immediately
```

**Effort**: 20-25 hours

---

### 4. Adaptive Partitioning

**Use Case**: Auto-adjust partition size based on data distribution

```bash
# Monitor partition sizes during creation
# Re-partition if imbalance detected
# Optimize for uniform work distribution
```

**Effort**: 15-20 hours

---

### 5. Distributed Partitioning

**Use Case**: Multi-node clusters

```bash
# Partition data across multiple machines
# Use network storage (NFS, HDFS)
# Coordinate with cluster manager (SLURM, Kubernetes)
```

**Effort**: 30-40 hours

---

## Decision Matrix

### Which Approach When?

| Scenario | Recommended Approach | Reason |
|----------|---------------------|--------|
| **Development/Testing** | Prolog Target | Fast, full features |
| **Small datasets (< 1GB)** | Any | Performance similar |
| **Large datasets (10GB+)** | Pure Bash + Compression | Efficient file I/O |
| **Complex logic** | Prolog Target or PaaS | Prolog excels here |
| **Production deployment** | Pure Bash | Zero dependencies |
| **Quick prototyping** | Prolog Target | Easiest to generate |
| **Distributed systems** | Pure Bash + Redis | Scalable |
| **Minimal systems** | Pure Bash | No Prolog required |

---

## Summary & Recommendation

### Recommended Implementation Order

1. **Phase 1**: Prolog as Target (~4 weeks)
   - Easiest, fastest ROI
   - Enables all features immediately
   - Foundation for later phases

2. **Phase 2**: Prolog-as-a-Service (~5 weeks)
   - Bridge to bash world
   - Still leverages Prolog strengths
   - More bash-friendly

3. **Phase 3**: Pure Bash Partitioning (~7 weeks)
   - Full portability
   - Zero dependencies
   - True transpilation goal

4. **Phase 4**: Optimizations (~5 weeks)
   - Performance tuning
   - Enterprise features
   - Production-ready

### Total Timeline: ~21 weeks (5 months)

### Incremental Delivery

Each phase delivers working functionality:
- **v0.0.3**: Prolog target (users can start using)
- **v0.0.4**: Bash/Prolog hybrid (more portable)
- **v0.0.5**: Pure bash (fully portable)
- **v0.1.0**: Optimized (production-ready)

---

## Open Questions

1. **Should we support PowerShell partitioning too?**
   - Similar to bash approach, but PowerShell-specific
   - Effort: +30 hours per phase

2. **Should Redis be bundled or optional?**
   - Optional: Less complexity, users install if needed
   - Bundled: Better UX, more dependencies

3. **Should we support S3/cloud storage for partitions?**
   - Useful for distributed systems
   - Effort: +20 hours

4. **Should partition files use custom format or CSV/JSON?**
   - Custom: Faster, less overhead
   - CSV/JSON: Interoperable, debuggable

5. **Should we implement partition caching?**
   - Reuse partitions across runs
   - Useful for iterative workflows
   - Effort: +15 hours

---

**Next Steps:**
1. Review this roadmap
2. Approve implementation order
3. Start Phase 1: Create Prolog target generator

---

**Related Documents:**
- `docs/proposals/prolog_as_target_language.md` - Prolog target detailed design
- `docs/proposals/partitioning_strategies.md` - Strategy algorithms
- `docs/proposals/parallel_backend_implementation_plan.md` - Backend comparison

**Version:** 1.0
**Last Updated:** 2025-10-27
