#!/usr/bin/env python3
"""
Train Minimal Transformation for Pearltrees.

This script:
1. Loads the Pearltrees Q/A pairs (generated by pearltrees_target_generator.py).
2. Embeds the queries (Short titles) and answers (Full materialized paths).
3. Trains a MinimalTransformProjection (Procrustes) to map Query -> Path.
4. Saves the learned projection matrix.
"""

import sys
import json
import logging
import numpy as np
from pathlib import Path
from typing import List, Dict
import pickle

# Add src to path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root / "src" / "unifyweaver" / "targets" / "python_runtime"))

from minimal_transform import MinimalTransformProjection

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def load_data(jsonl_path: Path, limit: int = None) -> List[Dict]:
    """Load Q/A pairs from JSONL."""
    data = []
    with open(jsonl_path, 'r') as f:
        for i, line in enumerate(f):
            if limit and i >= limit:
                break
            if line.strip():
                data.append(json.loads(line))
    return data

class SimpleEmbedder:
    """Wrapper for SentenceTransformers."""
    def __init__(self, model_name: str = "nomic-ai/nomic-embed-text-v1.5"):
        try:
            from sentence_transformers import SentenceTransformer
            # ModernBERT (nomic-embed-text-v1.5) requires trust_remote_code=True
            self.model = SentenceTransformer(model_name, trust_remote_code=True)
        except ImportError:
            logger.error("sentence-transformers not installed. Please install it to run this script.")
            sys.exit(1)

    def encode(self, texts: List[str]) -> np.ndarray:
        return self.model.encode(texts, show_progress_bar=True, convert_to_numpy=True)

def main():
    import argparse
    parser = argparse.ArgumentParser(description="Train federated W projection matrices")
    parser.add_argument("data", type=Path, help="Path to targets JSONL file")
    parser.add_argument("output", type=Path, help="Output model path (.pkl)")
    parser.add_argument("--model", type=str, default="nomic-ai/nomic-embed-text-v1.5",
                       help="Embedding model to use (default: nomic-ai/nomic-embed-text-v1.5)")
    parser.add_argument("--limit", type=int, default=None, help="Limit number of items to load")
    args = parser.parse_args()

    data_path = args.data
    output_path = args.output
    limit = args.limit

    logger.info(f"Loading data from {data_path}...")
    data = load_data(data_path, limit)
    logger.info(f"Loaded {len(data)} items.")

    # Prepare texts
    queries = [d['query'] for d in data]
    answers = [d['target_text'] for d in data]

    # Embed with specified model
    logger.info(f"Using embedding model: {args.model}")
    embedder = SimpleEmbedder(args.model)
    
    logger.info("Embedding queries...")
    Q_emb = embedder.encode(queries)
    
    logger.info("Embedding answers (paths)...")
    A_emb = embedder.encode(answers)

    # Group by cluster_id
    from collections import defaultdict
    cluster_groups = defaultdict(list)
    
    logger.info("Grouping data by cluster (parent tree)...")
    for i, d in enumerate(data):
        c_id = d.get('cluster_id', 'default')
        cluster_groups[c_id].append(i)
        
    clusters = []
    # Statistics
    logger.info(f"Found {len(cluster_groups)} unique clusters (trees).")
    
    # Prepare (Q, A) tuples for each cluster
    for c_id, indices in cluster_groups.items():
        if not indices:
            continue
            
        # Get subset of embeddings
        Q_subset = Q_emb[indices]
        A_subset = A_emb[indices]
        
        clusters.append((Q_subset, A_subset))
    
    logger.info(f"Training Minimal Transform Projection on {len(clusters)} clusters...")
    projector = MinimalTransformProjection(
        smooth_method="none",  # Pure Procrustes per-tree, no smoothing needed with ModernBERT
        fidelity_weight=1.0,   # Use exact minimal transforms
        allow_scaling=True
    )
    
    stats = projector.train(clusters)
    logger.info(f"Training stats: {stats}")
    
    # Evaluate on training set
    logger.info("Evaluating...")
    
    # Project using the full routing logic (automatically handles cluster selection via softmax)
    # We can't just use W_final[0] anymore.
    
    sims = []
    for i in range(len(queries)):
        q_vec = Q_emb[i]
        target_vec = A_emb[i]
        
        # Project using the trained multi-cluster model
        proj_vec = projector.project(q_vec)
        
        sim = np.dot(proj_vec, target_vec) / (np.linalg.norm(proj_vec) * np.linalg.norm(target_vec) + 1e-8)
        sims.append(sim)
    
    logger.info(f"Mean Cosine Similarity (Train - Softmax Routed): {np.mean(sims):.4f}")
    
    # Save using numpy's npz format (much more memory efficient)
    logger.info("Preparing model for saving...")
    
    # Extract what we need FIRST
    W_keys = sorted(projector.W_final.keys())
    centroids_list = list(projector.centroids)
    temperature = projector.temperature
    num_clusters = len(projector.centroids)
    
    # Save target embeddings and identifiers for fast inference
    # This avoids re-embedding targets at inference time
    A_emb_save = A_emb.astype(np.float32)
    target_ids = [d.get('tree_id', d.get('uri', str(i))) for i, d in enumerate(data)]
    target_titles = [d.get('raw_title', '') for d in data]
    
    # FREE MEMORY: Delete large objects we no longer need
    del Q_emb
    del A_emb
    del sims
    del cluster_groups
    del clusters
    
    import gc
    gc.collect()
    
    logger.info(f"Building stack of {len(W_keys)} matrices...")
    
    # Build the stack one matrix at a time to minimize peak memory
    first_W = projector.W_final[W_keys[0]]
    embedding_dim = first_W.shape[0]
    W_stack = np.zeros((len(W_keys), embedding_dim, embedding_dim), dtype=np.float32)
    
    for i, k in enumerate(W_keys):
        W_stack[i] = projector.W_final[k].astype(np.float32)
        del projector.W_final[k]  # Free as we go
        if i % 50 == 0:
            gc.collect()
    
    centroids_np = np.stack(centroids_list).astype(np.float32)
    del projector
    gc.collect()
    
    # Save as npz (compressed numpy archive)
    npz_path = str(output_path).replace('.pkl', '.npz')
    logger.info(f"Saving to {npz_path}...")
    np.savez_compressed(
        npz_path,
        W_stack=W_stack,
        centroids=centroids_np,
        temperature=np.array([temperature]),
        target_embeddings=A_emb_save,  # Pre-computed target embeddings
    )
    
    del W_stack
    del centroids_np
    del A_emb_save
    gc.collect()
    
    # Save lightweight metadata as pickle
    metadata = {
        "num_clusters": num_clusters,
        "embedding_dim": embedding_dim,
        "num_targets": len(target_ids),
        "target_ids": target_ids,
        "target_titles": target_titles,
        "stats": stats,
        "npz_file": npz_path
    }
    with open(output_path, 'wb') as f:
        pickle.dump(metadata, f)
        
    logger.info(f"Model saved: {output_path} (metadata) + {npz_path} (weights + target embeddings)")

if __name__ == "__main__":
    main()
