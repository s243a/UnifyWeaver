#!/usr/bin/env python3
"""
Train Minimal Transformation for Pearltrees.

This script:
1. Loads the Pearltrees Q/A pairs (generated by pearltrees_target_generator.py).
2. Embeds the queries (Short titles) and answers (Full materialized paths).
3. Trains a MinimalTransformProjection (Procrustes) to map Query -> Path.
4. Saves the learned projection matrix.
"""

import sys
import json
import logging
import numpy as np
from pathlib import Path
from typing import List, Dict
import pickle

# Add src to path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root / "src" / "unifyweaver" / "targets" / "python_runtime"))

from minimal_transform import MinimalTransformProjection

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def load_data(jsonl_path: Path, limit: int = None) -> List[Dict]:
    """Load Q/A pairs from JSONL."""
    data = []
    with open(jsonl_path, 'r') as f:
        for i, line in enumerate(f):
            if limit and i >= limit:
                break
            if line.strip():
                data.append(json.loads(line))
    return data

class SimpleEmbedder:
    """Wrapper for SentenceTransformers."""
    def __init__(self, model_name: str = "nomic-ai/nomic-embed-text-v1.5"):
        try:
            from sentence_transformers import SentenceTransformer
            # ModernBERT (nomic-embed-text-v1.5) requires trust_remote_code=True
            self.model = SentenceTransformer(model_name, trust_remote_code=True)
        except ImportError:
            logger.error("sentence-transformers not installed. Please install it to run this script.")
            sys.exit(1)

    def encode(self, texts: List[str]) -> np.ndarray:
        return self.model.encode(texts, show_progress_bar=True, convert_to_numpy=True)

def main():
    if len(sys.argv) < 3:
        print("Usage: python train_pearltrees_projection.py <targets.jsonl> <output_model.pkl> [limit]")
        sys.exit(1)

    data_path = Path(sys.argv[1])
    output_path = Path(sys.argv[2])
    limit = int(sys.argv[3]) if len(sys.argv) > 3 else None

    logger.info(f"Loading data from {data_path}...")
    data = load_data(data_path, limit)
    logger.info(f"Loaded {len(data)} items.")

    # Prepare texts
    queries = [d['query'] for d in data]
    answers = [d['target_text'] for d in data]

    # Embed
    embedder = SimpleEmbedder("nomic-ai/nomic-embed-text-v1.5")
    
    logger.info("Embedding queries...")
    Q_emb = embedder.encode(queries)
    
    logger.info("Embedding answers (paths)...")
    A_emb = embedder.encode(answers)

    # Train Projection
    # MinimalTransformProjection expects list of (Q, A) tuples (clusters).
    # Since we are doing "Zero-Shot" mapping where each item is unique,
    # we can treat the whole dataset as a single "cluster" or pairs.
    # However, Procrustes works best when mapping a distribution to another.
    # If we want a global linear map (or affine), we treat it as one giant cluster.
    
    logger.info("Training Minimal Transform Projection...")
    projector = MinimalTransformProjection(
        smooth_method="none",  # No smoothing needed for global fit
        allow_scaling=True,
        fidelity_weight=1.0 # Pure minimal transform
    )
    
    # We pass the entire dataset as one "cluster" to find the global best 
    # rotation + scaling that maps Query Space -> Answer Space.
    clusters = [(Q_emb, A_emb)]
    
    stats = projector.train(clusters)
    logger.info(f"Training stats: {stats}")
    
    # Evaluate on training set
    logger.info("Evaluating...")
    projected_Q = Q_emb @ projector.W_final[0] # Use the single learned matrix
    
    # Compute average cosine similarity
    norms_p = np.linalg.norm(projected_Q, axis=1)
    norms_a = np.linalg.norm(A_emb, axis=1)
    dots = np.sum(projected_Q * A_emb, axis=1)
    sims = dots / (norms_p * norms_a + 1e-8)
    
    logger.info(f"Mean Cosine Similarity (Train): {np.mean(sims):.4f}")
    
    # Save model
    model_data = {
        "projector": projector,
        "W": projector.W_final[0],
        "stats": stats
    }
    
    with open(output_path, 'wb') as f:
        pickle.dump(model_data, f)
    logger.info(f"Model saved to {output_path}")

if __name__ == "__main__":
    main()
